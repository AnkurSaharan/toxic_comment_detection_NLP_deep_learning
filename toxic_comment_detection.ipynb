{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Input,GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D,MaxPooling1D,Embedding\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH=100\n",
    "MAX_VOCAB_SIZE=20000\n",
    "EMBEDDING_DIM=100\n",
    "VALIDATION_SPLIT=0.2\n",
    "BATCH_SIZE=120\n",
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word2vec={}\n",
    "with open(os.path.join('glove.6B/glove.6B.%sd.txt' %EMBEDDING_DIM),encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vec=np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word]=vec\n",
    "print('found %s word vectors.' % len(word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sandberger\n"
     ]
    }
   ],
   "source": [
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28365   -0.6263    -0.44351    0.2177    -0.087421  -0.17062\n",
      "  0.29266   -0.024899   0.26414   -0.17023    0.25817    0.097484\n",
      " -0.33103   -0.43859    0.0095799  0.095624  -0.17777    0.38886\n",
      "  0.27151    0.14742   -0.43973   -0.26588   -0.024271   0.27186\n",
      " -0.36761   -0.24827   -0.20815    0.22128   -0.044409   0.021373\n",
      "  0.24594    0.26143    0.29303    0.13281    0.082232  -0.12869\n",
      "  0.1622    -0.22567   -0.060348   0.28703    0.11381    0.34839\n",
      "  0.3419     0.36996   -0.13592    0.0062694  0.080317   0.0036251\n",
      "  0.43093    0.01882    0.31008    0.16722    0.074112  -0.37745\n",
      "  0.47363    0.41284    0.24471    0.075965  -0.51725   -0.49481\n",
      "  0.526     -0.074645   0.41434   -0.1956    -0.16544   -0.045649\n",
      " -0.40153   -0.13136   -0.4672     0.18825    0.2612     0.16854\n",
      "  0.22615    0.62992   -0.1288     0.055841   0.01928    0.024572\n",
      "  0.46875    0.2582    -0.31672    0.048591   0.3277    -0.50141\n",
      "  0.30855    0.11997   -0.25768   -0.039867  -0.059672   0.5525\n",
      "  0.13885   -0.22862    0.071792  -0.43208    0.5398    -0.085806\n",
      "  0.032651   0.43678   -0.82607   -0.15701  ]\n"
     ]
    }
   ],
   "source": [
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28365   -0.6263    -0.44351    0.2177    -0.087421  -0.17062\n",
      "  0.29266   -0.024899   0.26414   -0.17023    0.25817    0.097484\n",
      " -0.33103   -0.43859    0.0095799  0.095624  -0.17777    0.38886\n",
      "  0.27151    0.14742   -0.43973   -0.26588   -0.024271   0.27186\n",
      " -0.36761   -0.24827   -0.20815    0.22128   -0.044409   0.021373\n",
      "  0.24594    0.26143    0.29303    0.13281    0.082232  -0.12869\n",
      "  0.1622    -0.22567   -0.060348   0.28703    0.11381    0.34839\n",
      "  0.3419     0.36996   -0.13592    0.0062694  0.080317   0.0036251\n",
      "  0.43093    0.01882    0.31008    0.16722    0.074112  -0.37745\n",
      "  0.47363    0.41284    0.24471    0.075965  -0.51725   -0.49481\n",
      "  0.526     -0.074645   0.41434   -0.1956    -0.16544   -0.045649\n",
      " -0.40153   -0.13136   -0.4672     0.18825    0.2612     0.16854\n",
      "  0.22615    0.62992   -0.1288     0.055841   0.01928    0.024572\n",
      "  0.46875    0.2582    -0.31672    0.048591   0.3277    -0.50141\n",
      "  0.30855    0.11997   -0.25768   -0.039867  -0.059672   0.5525\n",
      "  0.13885   -0.22862    0.071792  -0.43208    0.5398    -0.085806\n",
      "  0.032651   0.43678   -0.82607   -0.15701  ]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec['sandberger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('jigsaw-toxic-comment-classification-challenge/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('jigsaw-toxic-comment-classification-challenge/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=train[\"comment_text\"].fillna(\"dummy_value\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences=test[\"comment_text\"].fillna(\"dummy_value\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       "       \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",\n",
       "       \"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",\n",
       "       ...,\n",
       "       'Spitzer \\n\\nUmm, theres no actual article for prostitution ring.  - Crunch Captain.',\n",
       "       'And it looks like it was actually you who put on the speedy to have the first version deleted now that I look at it.',\n",
       "       '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=train[possible_labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sequence length: 5000\n",
      "min sequence length: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"max sequence length:\",max(len(s) for s in sentences))\n",
    "print(\"min sequence length:\",min(len(s) for s in sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median sequence length: 205\n"
     ]
    }
   ],
   "source": [
    "s=sorted(len(s) for s in sentences)\n",
    "print(\"median sequence length:\",s[len(s) // 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences=tokenizer.texts_to_sequences(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(test_sentences)\n",
    "test_sequences=tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394787 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word2idx=tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (159571, 100)\n"
     ]
    }
   ],
   "source": [
    "data=pad_sequences(sequences,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:',data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=min(MAX_VOCAB_SIZE,len(word2idx)+1)\n",
    "embedding_matrix=np.zeros((num_words,EMBEDDING_DIM))\n",
    "for word,i in word2idx.items():\n",
    "    if i<MAX_VOCAB_SIZE:\n",
    "        embedding_vector=word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=MAX_SEQUENCE_LENGTH,\n",
    "  trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_=Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x=embedding_layer(input_)\n",
    "x=Conv1D(128,3,activation='relu')(x)\n",
    "x=MaxPooling1D(3)(x)\n",
    "x=Conv1D(128,3,activation='relu')(x)\n",
    "x=MaxPooling1D(3)(x)\n",
    "x=Conv1D(128,3,activation='relu')(x)\n",
    "x=GlobalMaxPooling1D()(x)\n",
    "x=Dense(128,activation='relu')(x)\n",
    "output=Dense(len(possible_labels),activation='sigmoid')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(input_,output)\n",
    "model.compile(\n",
    " loss='binary_crossentropy',\n",
    " optimizer='rmsprop',\n",
    " metrics =['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ankur Saharan\\Anaconda3new\\envs\\AnkurEnv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Ankur Saharan\\Anaconda3new\\envs\\AnkurEnv\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/15\n",
      "127656/127656 [==============================] - 144s 1ms/step - loss: 0.1104 - acc: 0.9683 - val_loss: 0.0964 - val_acc: 0.9729\n",
      "Epoch 2/15\n",
      "127656/127656 [==============================] - 138s 1ms/step - loss: 0.0865 - acc: 0.9740 - val_loss: 0.0852 - val_acc: 0.9737\n",
      "Epoch 3/15\n",
      "127656/127656 [==============================] - 140s 1ms/step - loss: 0.0789 - acc: 0.9755 - val_loss: 0.0828 - val_acc: 0.9747\n",
      "Epoch 4/15\n",
      "127656/127656 [==============================] - 135s 1ms/step - loss: 0.0740 - acc: 0.9766 - val_loss: 0.0978 - val_acc: 0.9709\n",
      "Epoch 5/15\n",
      "127656/127656 [==============================] - 127s 995us/step - loss: 0.0707 - acc: 0.9774 - val_loss: 0.0834 - val_acc: 0.9740\n",
      "Epoch 6/15\n",
      "127656/127656 [==============================] - 115s 904us/step - loss: 0.0677 - acc: 0.9782 - val_loss: 0.0865 - val_acc: 0.9749\n",
      "Epoch 7/15\n",
      "127656/127656 [==============================] - 116s 906us/step - loss: 0.0652 - acc: 0.9786 - val_loss: 0.0859 - val_acc: 0.9743\n",
      "Epoch 8/15\n",
      "127656/127656 [==============================] - 131s 1ms/step - loss: 0.0631 - acc: 0.9793 - val_loss: 0.0959 - val_acc: 0.9748\n",
      "Epoch 9/15\n",
      "127656/127656 [==============================] - 122s 956us/step - loss: 0.0613 - acc: 0.9794 - val_loss: 0.0981 - val_acc: 0.9746\n",
      "Epoch 10/15\n",
      "127656/127656 [==============================] - 112s 878us/step - loss: 0.0596 - acc: 0.9798 - val_loss: 0.0983 - val_acc: 0.9750\n",
      "Epoch 11/15\n",
      "127656/127656 [==============================] - 113s 886us/step - loss: 0.0586 - acc: 0.9800 - val_loss: 0.0961 - val_acc: 0.9751\n",
      "Epoch 12/15\n",
      "127656/127656 [==============================] - 132s 1ms/step - loss: 0.0574 - acc: 0.9803 - val_loss: 0.0981 - val_acc: 0.9747\n",
      "Epoch 13/15\n",
      "127656/127656 [==============================] - 119s 936us/step - loss: 0.0565 - acc: 0.9805 - val_loss: 0.0977 - val_acc: 0.9742\n",
      "Epoch 14/15\n",
      "127656/127656 [==============================] - 134s 1ms/step - loss: 0.0558 - acc: 0.9808 - val_loss: 0.1003 - val_acc: 0.9721\n",
      "Epoch 15/15\n",
      "127656/127656 [==============================] - 134s 1ms/step - loss: 0.0552 - acc: 0.9809 - val_loss: 0.0983 - val_acc: 0.9746\n"
     ]
    }
   ],
   "source": [
    "r=model.fit(\n",
    "data,\n",
    "targets,\n",
    "batch_size=BATCH_SIZE,\n",
    "epochs=15,\n",
    "validation_split=VALIDATION_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00]\n",
      " [1.7197728e-03 0.0000000e+00 7.5131655e-05 0.0000000e+00 2.4408102e-05\n",
      "  8.9406967e-08]\n",
      " [2.9802322e-08 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00]\n",
      " [2.6029348e-03 0.0000000e+00 1.3208389e-04 0.0000000e+00 6.0617924e-05\n",
      "  1.6391277e-06]\n",
      " [7.2511435e-02 3.6305189e-04 2.0640254e-02 9.0429187e-04 1.8567771e-02\n",
      "  3.4393668e-03]\n",
      " [5.0240695e-02 7.1078539e-05 9.0350807e-03 4.6932697e-04 8.4850192e-03\n",
      "  1.4385581e-03]\n",
      " [6.2775314e-03 1.1920929e-07 2.8732419e-04 8.0466270e-07 1.9723177e-04\n",
      "  7.8082085e-06]\n",
      " [1.6133666e-02 1.2516975e-06 2.3224950e-03 2.6822090e-07 1.1781156e-03\n",
      "  1.4275312e-05]\n",
      " [9.9197447e-02 1.3095140e-04 2.1187067e-02 9.4866753e-04 1.8467367e-02\n",
      "  3.6202967e-03]\n",
      " [1.7294884e-03 0.0000000e+00 4.3213367e-05 0.0000000e+00 7.6293945e-06\n",
      "  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(res[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.DataFrame(res)\n",
    "#result_toxic_comment=pd.to_csv(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
